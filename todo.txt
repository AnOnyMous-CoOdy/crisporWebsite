JP Sep 23 14:
- change ranking formula
- site en francais?
- show restriction sites

- show 12 bp mismatches
- keep lowercase characters in input sequence
- cloning helper

sgRNA activity predictive model (copié du M&M sur le site)


Within each gene, passing sgRNAs were first ranked, with the best sgRNA
receiving the rank of 1. This number was then divided by the total number of
sgRNAs, which was then subtracted from 1 to determine a percent-rank. This
results in the worst sgRNA for a gene receiving a percent-rank of 0, while the
best sgRNA will have a percent-rank approaching 1. Percent-rank values were
averaged for genes that were assayed in more than one cell line.

The features used for prediction were the individual nucleotides and all pairs
of adjacent nucleotides indexed by position in the 30 mer target site. We also
included the count of Gs and Cs in the 20 nt of the sgRNA. Because of an
observed nonlinear dependence between GC content and efficacy, two GC-count
features were also incorporated: one for deviations below ten and one for
deviations above ten. To allow for independent weights for each nucleotide
feature, the nucleotide feature space was represented with one-hot encoding.

Because the full set of features—with 120 single nucleotide features, 464
dinucleotide features, and the two GC-count features—was overdetermined, we
incorporated feature selection to choose a subset of features with the best
generalization error. An L1-regularized linear support vector machine (SVM)
implemented in the python module scikit-learn was used to generate sets of
features as a function of the L1-norm penalty. Given the set of features from
the SVM, a logistic regression classifier was trained to discriminate the top
quintile of sgRNAs for each gene from the remainder. We cross-validated the
model by training on the data for eight genes and predicting on the data for
the remaining gene. The feature selection step was run in a nested stratified
cross validation loop on the training data in which each fold excluded an equal
proportion of the sgRNAs for each of the eight training genes. The L1-norm
penalty was chosen to maximize the average holdout AUC in the nested loop. We
also used leave-one-sgRNA-out cross validation to measure the performance of
the model, though leave-one-gene-out is a more realistic measure of
generalization performance. After validation, we trained a final model using
all available data (Supplementary Table 9), which used only 72 of the 586
features, including both GC-count features.

The model weights presented in Supplementary Table 9 can be used to easily
compute the sgRNA score. A guide necessarily only has a subset of all the
features, indicated via one-hot encoding as binary variables. Let the model
weights for the features i for a particular guide sj be wij, the intercept int.
Then the sgRNA score f(sj) is given via logistic regression as:


Model scores f(sj) will fall into the range [0,1], and higher values predict
higher activity.




